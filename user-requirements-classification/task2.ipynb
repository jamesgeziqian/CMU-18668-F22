{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db54b33-ac51-4960-9f09-10d8551b2a6c",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654a2823-2ba6-4dfb-a9d5-2bb5275028b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664096c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 4\n",
    "N_JOBS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe29e0a5-21d4-480e-bc72-ba5c1fe6e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_label = 'stopwords_removal_nltk'\n",
    "label_name = 'label'\n",
    "senti_label = 'sentiScore'\n",
    "features = [text_label, senti_label, label_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5432c8",
   "metadata": {},
   "source": [
    "Parameters to tune for SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12df6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C\n",
    "C = [.0001, .001, .1, 1]\n",
    "\n",
    "# gamma\n",
    "gamma = [.0001, .001, .01, 1, 10, 100, 'auto']\n",
    "\n",
    "# degree\n",
    "degree = list(range(1, 5))\n",
    "\n",
    "# kernel\n",
    "kernel = ['rbf', 'poly']\n",
    "\n",
    "# probability\n",
    "probability = [True]\n",
    "\n",
    "# create a random grid\n",
    "random_grid_svc = {'C': C,\n",
    "                   'kernel': kernel,\n",
    "                   'gamma': gamma,\n",
    "                   'degree': degree,\n",
    "                   'probability': probability\n",
    "                   }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241393f7",
   "metadata": {},
   "source": [
    "Parameters to tune for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21cc24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid_forest = {'bootstrap': [True, False],\n",
    "                      'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "                      'max_features': ['log2', 'sqrt'],\n",
    "                      'min_samples_leaf': [1, 2, 4],\n",
    "                      'min_samples_split': [2, 5, 10],\n",
    "                      'n_jobs': [N_JOBS],\n",
    "                      'random_state': [RANDOM_STATE],\n",
    "                      'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "                      'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "                      }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdae38b",
   "metadata": {},
   "source": [
    "Tfidf parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e4a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM_RANGE = (1, 2)\n",
    "MIN_DF = .01  # ignore terms that appear in less than 1% of the documents\n",
    "MAX_DF = .8  # ignore terms that appear in more than 80% of the documents\n",
    "MAX_FEATURES = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e61a8",
   "metadata": {},
   "source": [
    "#### Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6724f339-1bcd-477b-a71c-d09a627aaea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    :param file_path: path to the json file\n",
    "\n",
    "    :return: an array in which each entry is tuple [review, classification label]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path) as json_file:\n",
    "        raw_data = json.load(json_file)\n",
    "    return convert_data(raw_data)\n",
    "\n",
    "\n",
    "def convert_data(raw_data):\n",
    "    extracted = [[elem[feat] for feat in features] for elem in raw_data]\n",
    "    return pd.DataFrame(extracted, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d4505d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bug = load_file('Bug_tt.json')\n",
    "data_feat = load_file('Feature_tt.json')\n",
    "data_rating = load_file('Rating_tt.json')\n",
    "data_ux = load_file('UserExperience_tt.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af52ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_label(data: pd.DataFrame, class_name, inplace=False):\n",
    "    new_label = np.where(data[label_name] == class_name, 1, 0)\n",
    "    new_data = data if inplace else data.copy()\n",
    "    new_data[label_name] = new_label\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9ce2b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopwords_removal_nltk</th>\n",
       "      <th>sentiScore</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>still need check each, think need improve tech...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>app helpful!!!! truly amazing!!!! paid bills</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awesome!!!</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love app takes less ten seconds let know batte...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it's app pretty much everything making list of...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>crashes lenovo tablet app keeps crashing loses...</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>terrible took forever download</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>norefund wtf seems like nice game working sgs2...</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>need speed wanted would good game would load i...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>already paid paid game want money whole experi...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>740 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                stopwords_removal_nltk  sentiScore  label\n",
       "0    still need check each, think need improve tech...           2      1\n",
       "1         app helpful!!!! truly amazing!!!! paid bills           4      1\n",
       "2                                           awesome!!!           4      1\n",
       "3    love app takes less ten seconds let know batte...           3      1\n",
       "4    it's app pretty much everything making list of...           3      1\n",
       "..                                                 ...         ...    ...\n",
       "735  crashes lenovo tablet app keeps crashing loses...          -2      0\n",
       "736                     terrible took forever download          -4      0\n",
       "737  norefund wtf seems like nice game working sgs2...          -3      0\n",
       "738  need speed wanted would good game would load i...           2      0\n",
       "739  already paid paid game want money whole experi...          -1      0\n",
       "\n",
       "[740 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_label(data_bug, 'Bug', inplace=True)\n",
    "transform_label(data_feat, 'Feature', inplace=True)\n",
    "transform_label(data_rating, 'Rating', inplace=True)\n",
    "transform_label(data_ux, 'UserExperience', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65b0ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bug, X_test_bug, y_train_bug, y_test_bug = train_test_split(\n",
    "    data_bug[[text_label, senti_label]], data_bug[label_name], test_size=.15, random_state=RANDOM_STATE)\n",
    "X_train_feat, X_test_feat, y_train_feat, y_test_feat = train_test_split(\n",
    "    data_feat[[text_label, senti_label]], data_feat[label_name], test_size=.15, random_state=RANDOM_STATE)\n",
    "X_train_rating, X_test_rating, y_train_rating, y_test_rating = train_test_split(\n",
    "    data_rating[[text_label, senti_label]], data_rating[label_name], test_size=.15, random_state=RANDOM_STATE)\n",
    "X_train_ux, X_test_ux, y_train_ux, y_test_ux = train_test_split(\n",
    "    data_ux[[text_label, senti_label]], data_ux[label_name], test_size=.15, random_state=RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b19ed6",
   "metadata": {},
   "source": [
    "## Native Mutliclass Algorithm (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eee850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_text(train, test):\n",
    "    tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            stop_words=None,\n",
    "                            lowercase=False,\n",
    "                            max_df=MAX_DF,\n",
    "                            min_df=MIN_DF,\n",
    "                            max_features=MAX_FEATURES,\n",
    "                            norm='l2',\n",
    "                            sublinear_tf=True)\n",
    "    features_train = tfidf.fit_transform(train[text_label]).toarray()\n",
    "    features_test = tfidf.transform(test[text_label]).toarray()\n",
    "    features_train = np.append(\n",
    "        features_train, train[[senti_label]].to_numpy(), axis=1)\n",
    "    features_test = np.append(\n",
    "        features_test, test[[senti_label]].to_numpy(), axis=1)\n",
    "    return features_train, features_test\n",
    "\n",
    "\n",
    "def drop_non_class(X, y):\n",
    "    df = X.copy()\n",
    "    df[label_name] = y\n",
    "    df = df.loc[y == 1, :]\n",
    "    return df[X.columns], df[label_name]\n",
    "\n",
    "\n",
    "def combine_datasets(X_bug, y_bug, X_feat, y_feat, X_rating, y_rating, X_ux, y_ux):\n",
    "    X_bug, y_bug = drop_non_class(X_bug, y_bug)\n",
    "    X_feat, y_feat = drop_non_class(X_feat, y_feat)\n",
    "    X_rating, y_rating = drop_non_class(X_rating, y_rating)\n",
    "    X_ux, y_ux = drop_non_class(X_ux, y_ux)\n",
    "    X = pd.concat([X_bug, X_feat, X_rating, X_ux], axis=0)\n",
    "    y = np.concatenate([np.zeros_like(y_bug),\n",
    "                        np.ones_like(y_feat),\n",
    "                        np.ones_like(y_rating) * 2,\n",
    "                        np.ones_like(y_ux) * 3])\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa5beea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopwords_removal_nltk</th>\n",
       "      <th>sentiScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>love nook ipad love many books always me somet...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>crashes update</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>please fix it's often used function</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>really wanted note system could trust, evernot...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>important add landscape sheet orientation word...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>love photos backed automatically keep phone</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>aside occasional glitch, pretty good awesome g...</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>everything wanted craigslist never thought of!...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>exactly says tin effecient enough check respon...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>link needed perfect solution android  linux en...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1183 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                stopwords_removal_nltk  sentiScore\n",
       "65   love nook ipad love many books always me somet...           3\n",
       "276                                     crashes update          -1\n",
       "70                 please fix it's often used function           2\n",
       "337  really wanted note system could trust, evernot...          -1\n",
       "62   important add landscape sheet orientation word...          -1\n",
       "..                                                 ...         ...\n",
       "109        love photos backed automatically keep phone           3\n",
       "58   aside occasional glitch, pretty good awesome g...          -3\n",
       "360  everything wanted craigslist never thought of!...           3\n",
       "174  exactly says tin effecient enough check respon...          -1\n",
       "122  link needed perfect solution android  linux en...           2\n",
       "\n",
       "[1183 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_combined, y_train_combined = combine_datasets(\n",
    "    X_train_bug, y_train_bug, X_train_feat, y_train_feat, X_train_rating, y_train_rating, X_train_ux, y_train_ux)\n",
    "X_train_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e78bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopwords_removal_nltk</th>\n",
       "      <th>sentiScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>used used app, it's pain find hotels searching...</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>need improve functionality app can't vote revi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>liked much upgrade pdfs (divisions search) how...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>finebt cud b improved!! d popular app smartphn...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>update makes harder slower use icons different...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>far best organizational app around use dozens ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>best! really like features especially stickers ?</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>enjoy sending verses friends iphone</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>good ,i like it accurate  high tech</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>amazing!! ive using almost 1yr!!!!!! omgyou mu...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                stopwords_removal_nltk  sentiScore\n",
       "104  used used app, it's pain find hotels searching...          -4\n",
       "83   need improve functionality app can't vote revi...           2\n",
       "17   liked much upgrade pdfs (divisions search) how...           3\n",
       "139  finebt cud b improved!! d popular app smartphn...           3\n",
       "303  update makes harder slower use icons different...          -1\n",
       "..                                                 ...         ...\n",
       "253  far best organizational app around use dozens ...           2\n",
       "249   best! really like features especially stickers ?           3\n",
       "146                enjoy sending verses friends iphone           3\n",
       "358                good ,i like it accurate  high tech           2\n",
       "224  amazing!! ive using almost 1yr!!!!!! omgyou mu...           4\n",
       "\n",
       "[222 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_combined, y_test_combined = combine_datasets(\n",
    "    X_test_bug, y_test_bug, X_test_feat, y_test_feat, X_test_rating, y_test_rating, X_test_ux, y_test_ux)\n",
    "X_test_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aca9b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183, 301)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_combined_vec_senti, X_test_combined_vec_senti = vectorized_text(X_train_combined, X_test_combined)\n",
    "X_train_combined_vec_senti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71be626c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=50,\n",
       "                   n_jobs=8,\n",
       "                   param_distributions={&#x27;bootstrap&#x27;: [True, False],\n",
       "                                        &#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;,\n",
       "                                                      &#x27;log_loss&#x27;],\n",
       "                                        &#x27;max_depth&#x27;: [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, None],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;log2&#x27;, &#x27;sqrt&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000],\n",
       "                                        &#x27;n_jobs&#x27;: [8], &#x27;random_state&#x27;: [4]},\n",
       "                   random_state=4, scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=50,\n",
       "                   n_jobs=8,\n",
       "                   param_distributions={&#x27;bootstrap&#x27;: [True, False],\n",
       "                                        &#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;,\n",
       "                                                      &#x27;log_loss&#x27;],\n",
       "                                        &#x27;max_depth&#x27;: [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, None],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;log2&#x27;, &#x27;sqrt&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000],\n",
       "                                        &#x27;n_jobs&#x27;: [8], &#x27;random_state&#x27;: [4]},\n",
       "                   random_state=4, scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=50,\n",
       "                   n_jobs=8,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'criterion': ['gini', 'entropy',\n",
       "                                                      'log_loss'],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, None],\n",
       "                                        'max_features': ['log2', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000],\n",
       "                                        'n_jobs': [8], 'random_state': [4]},\n",
       "                   random_state=4, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_forest = RandomForestClassifier()\n",
    "random_cv_forest = RandomizedSearchCV(estimator=clf_forest,\n",
    "                                      param_distributions=random_grid_forest,\n",
    "                                      n_iter=50,\n",
    "                                      scoring='accuracy',\n",
    "                                      cv=3,\n",
    "                                      verbose=1,\n",
    "                                      n_jobs=N_JOBS,\n",
    "                                      random_state=RANDOM_STATE)\n",
    "random_cv_forest.fit(X_train_combined_vec_senti, y_train_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e54a8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Forest Multiclass\n",
      "Accuracy: 0.5765765765765766\n",
      "F1 Score: [0.60465116 0.31428571 0.63492063 0.63865546]\n",
      "Precision: [0.55714286 0.44       0.59701493 0.63333333]\n",
      "Recall: [0.66101695 0.24444444 0.6779661  0.6440678 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_combined = random_cv_forest.predict(X_test_combined_vec_senti)\n",
    "print(f'''Testing Forest Multiclass\n",
    "Accuracy: {accuracy_score(y_test_combined, predicted_combined)}\n",
    "F1 Score: {f1_score(y_test_combined, predicted_combined, average=None)}\n",
    "Precision: {precision_score(y_test_combined, predicted_combined, average=None)}\n",
    "Recall: {recall_score(y_test_combined, predicted_combined, average=None)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d3a5f",
   "metadata": {},
   "source": [
    "## One vs Rest method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0f6a5",
   "metadata": {},
   "source": [
    "An implementation of one-vs-rest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6ac3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OVRClassifier:\n",
    "    def __init__(self, clf_type='svc') -> None:\n",
    "        self.clf_type = clf_type\n",
    "        self.bug_scaler = StandardScaler()\n",
    "        self.bug_clf = self._make_clf()\n",
    "        self.bug_vectorizer = self._make_tfidf()\n",
    "        self.feat_scaler = StandardScaler()\n",
    "        self.feat_clf = self._make_clf()\n",
    "        self.feat_vectorizer = self._make_tfidf()\n",
    "        self.rating_scaler = StandardScaler()\n",
    "        self.rating_clf = self._make_clf()\n",
    "        self.rating_vectorizer = self._make_tfidf()\n",
    "        self.ux_scaler = StandardScaler()\n",
    "        self.ux_clf = self._make_clf()\n",
    "        self.ux_vectorizer = self._make_tfidf()\n",
    "        self.random_grid = {'svc': random_grid_svc,\n",
    "                            'forest': random_grid_forest}\n",
    "\n",
    "    def _make_tfidf(self, ngram_range=NGRAM_RANGE, max_df=MAX_DF, min_df=MIN_DF, max_features=MAX_FEATURES):\n",
    "        return TfidfVectorizer(encoding='utf-8',\n",
    "                               ngram_range=ngram_range,\n",
    "                               stop_words=None,\n",
    "                               lowercase=False,\n",
    "                               max_df=max_df,\n",
    "                               min_df=min_df,\n",
    "                               max_features=max_features,\n",
    "                               norm='l2',\n",
    "                               sublinear_tf=True)\n",
    "    \n",
    "    def _make_random_searcher_svc(self, clf):\n",
    "        return RandomizedSearchCV(estimator=clf,\n",
    "                                  param_distributions=self.random_grid[self.clf_type],\n",
    "                                  n_iter=50,\n",
    "                                  scoring='accuracy',\n",
    "                                  cv=3,\n",
    "                                  verbose=1,\n",
    "                                  n_jobs=N_JOBS,\n",
    "                                  random_state=RANDOM_STATE)\n",
    "    \n",
    "    def _make_clf(self):\n",
    "        if self.clf_type == 'svc':\n",
    "            return SVC()\n",
    "        elif self.clf_type == 'forest':\n",
    "            return RandomForestClassifier()\n",
    "        raise ValueError(f'Unknown classifier type: {self.clf_type}')\n",
    "\n",
    "    def _fit(self, X, y, vectorizer, scaler, clf):\n",
    "        X_vec = vectorizer.fit_transform(X[text_label]).toarray()\n",
    "        X_vec_senti = np.append(\n",
    "            X_vec, X[[senti_label]].to_numpy(), axis=1)\n",
    "        X_normalized = scaler.fit_transform(X_vec_senti)\n",
    "        random_cv = self._make_random_searcher_svc(clf)\n",
    "        random_cv.fit(X_normalized, y)\n",
    "        print(f'Training accuracy: {random_cv.best_score_}')\n",
    "        return random_cv.best_estimator_\n",
    "\n",
    "    def _fit_bug(self, X, y):\n",
    "        self.bug_clf = self._fit(X, y, self.bug_vectorizer,\n",
    "                                 self.bug_scaler, self.bug_clf)\n",
    "\n",
    "    def _fit_feat(self, X, y):\n",
    "        self.feat_clf = self._fit(X, y, self.feat_vectorizer,\n",
    "                                  self.feat_scaler, self.feat_clf)\n",
    "\n",
    "    def _fit_rating(self, X, y):\n",
    "        self.rating_clf = self._fit(X, y, self.rating_vectorizer,\n",
    "                                    self.rating_scaler, self.rating_clf)\n",
    "\n",
    "    def _fit_ux(self, X, y):\n",
    "        self.ux_clf = self._fit(X, y, self.ux_vectorizer,\n",
    "                                self.ux_scaler, self.ux_clf)\n",
    "\n",
    "    def _predict_proba(self, X, vectorizer, scaler, clf):\n",
    "        X_vec = vectorizer.transform(X[text_label]).toarray()\n",
    "        X_vec_senti = np.append(X_vec, X[[senti_label]].to_numpy(), axis=1)\n",
    "        X_normalized = scaler.transform(X_vec_senti)\n",
    "        return clf.predict_proba(X_normalized)\n",
    "\n",
    "    def _predict(self, X, vectorizer, scaler, clf):\n",
    "        predicted_proba = self._predict_proba(X, vectorizer, scaler, clf)\n",
    "        return np.argmax(predicted_proba, axis=1)\n",
    "    \n",
    "    def _predict_proba_bug(self, X):\n",
    "        return self._predict_proba(X, self.bug_vectorizer, self.bug_scaler, self.bug_clf)\n",
    "\n",
    "    def _predict_proba_feat(self, X):\n",
    "        return self._predict_proba(X, self.feat_vectorizer, self.feat_scaler, self.feat_clf)\n",
    "\n",
    "    def _predict_proba_rating(self, X):\n",
    "        return self._predict_proba(X, self.rating_vectorizer, self.rating_scaler, self.rating_clf)\n",
    "\n",
    "    def _predict_proba_ux(self, X):\n",
    "        return self._predict_proba(X, self.ux_vectorizer, self.ux_scaler, self.ux_clf)\n",
    "\n",
    "    def _predict_bug(self, X):\n",
    "        return self._predict(X, self.bug_vectorizer, self.bug_scaler, self.bug_clf)\n",
    "\n",
    "    def fit(self, X_bug, y_bug, X_feat, y_feat, X_rating, y_rating, X_ux, y_ux):\n",
    "        self._fit_bug(X_bug, y_bug)\n",
    "        self._fit_feat(X_feat, y_feat)\n",
    "        self._fit_rating(X_rating, y_rating)\n",
    "        self._fit_ux(X_ux, y_ux)\n",
    "\n",
    "    def predict(self, X):\n",
    "        bug_proba = self._predict_proba_bug(X)[:, 1]\n",
    "        feat_proba = self._predict_proba_feat(X)[:, 1]\n",
    "        rating_proba = self._predict_proba_rating(X)[:, 1]\n",
    "        ux_proba = self._predict_proba_ux(X)[:, 1]\n",
    "        return np.argmax([bug_proba, feat_proba, rating_proba, ux_proba], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "737246a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Training accuracy: 0.7312903470798208\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Training accuracy: 0.7532348111658456\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Training accuracy: 0.7630819472924736\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Training accuracy: 0.7790081263765475\n"
     ]
    }
   ],
   "source": [
    "ova_svc = OVRClassifier()\n",
    "ova_svc.fit(X_train_bug, y_train_bug, X_train_feat, y_train_feat,\n",
    "            X_train_rating, y_train_rating, X_train_ux, y_train_ux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f7965d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Training accuracy: 0.7503227766385662\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Training accuracy: 0.8048823207443897\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Training accuracy: 0.7694615326194274\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Training accuracy: 0.7869294448241817\n"
     ]
    }
   ],
   "source": [
    "ova_forest = OVRClassifier('forest')\n",
    "ova_forest.fit(X_train_bug, y_train_bug, X_train_feat, y_train_feat,\n",
    "               X_train_rating, y_train_rating, X_train_ux, y_train_ux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef02db06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SVC one-vs-rest\n",
      "Accuracy: 0.5585585585585585\n",
      "F1 Score: [0.55045872 0.51546392 0.57142857 0.58928571]\n",
      "Precision: [0.6        0.48076923 0.53731343 0.62264151]\n",
      "Recall: [0.50847458 0.55555556 0.61016949 0.55932203]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test_svc = ova_svc.predict(X_test_combined)\n",
    "print(f'''Testing SVC one-vs-rest\n",
    "Accuracy: {accuracy_score(y_test_combined, pred_test_svc)}\n",
    "F1 Score: {f1_score(y_test_combined, pred_test_svc, average=None)}\n",
    "Precision: {precision_score(y_test_combined, pred_test_svc, average=None)}\n",
    "Recall: {recall_score(y_test_combined, pred_test_svc, average=None)}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cf746c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Random Forest one-vs-rest\n",
      "Accuracy: 0.527027027027027\n",
      "F1 Score: [0.3956044  0.48739496 0.61538462 0.57692308]\n",
      "Precision: [0.5625     0.39189189 0.56338028 0.66666667]\n",
      "Recall: [0.30508475 0.64444444 0.6779661  0.50847458]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test_forest = ova_forest.predict(X_test_combined)\n",
    "print(f'''Testing Random Forest one-vs-rest\n",
    "Accuracy: {accuracy_score(y_test_combined, pred_test_forest)}\n",
    "F1 Score: {f1_score(y_test_combined, pred_test_forest, average=None)}\n",
    "Precision: {precision_score(y_test_combined, pred_test_forest, average=None)}\n",
    "Recall: {recall_score(y_test_combined, pred_test_forest, average=None)}\n",
    "''')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('dsse')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "98261eef3a20da3da6c7c1298e3df5d537cf7b33b402c8caf32440f7f613ecab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
